# Web Crawler Configuration File
# 
# This file contains default settings and configurations for the web crawler.
# You can modify these settings to customize the crawler behavior.

[DEFAULT_SETTINGS]
# Default keywords to search for (comma-separated)
keywords = python,web,scrapy,data,programming

# Default target domain (leave empty to allow any domain)
target_domain = 

# Default starting URL
start_url = http://quotes.toscrape.com

# Maximum crawl depth (0 = unlimited, but not recommended)
max_depth = 3

# Output directory for results
output_dir = output

# Whether to filter pages by keywords (true/false)
filter_by_keywords = true

# Download delay between requests (seconds)
download_delay = 1.0

# Maximum concurrent requests
concurrent_requests = 16

# User agent string
user_agent = WebCrawler/1.0 (+https://github.com/yourproject)

[ADVANCED_SETTINGS]
# Respect robots.txt (true/false)
obey_robotstxt = true

# Enable auto-throttling (adjusts delay based on server response)
autothrottle_enabled = true

# Enable HTTP caching (speeds up repeated runs)
http_cache_enabled = false

# Maximum file size to download (in bytes, 0 = unlimited)
download_maxsize = 1048576

# Timeout for downloads (seconds)
download_timeout = 180

# Enable cookies (true/false)
cookies_enabled = false

[EXPORT_SETTINGS]
# Export formats (csv, json, both)
export_format = both

# Include full content in exports (true/false)
include_full_content = true

# Maximum content length in CSV preview
csv_content_preview_length = 200

# Timestamp format for filenames
timestamp_format = %Y%m%d_%H%M%S

[KEYWORD_SETTINGS]
# Case sensitive keyword matching (true/false)
case_sensitive = false

# Minimum keyword matches required to keep a page
min_keyword_matches = 1

# Search in page title (true/false)
search_in_title = true

# Search in page content (true/false)  
search_in_content = true

# Search in meta description (true/false)
search_in_meta = true