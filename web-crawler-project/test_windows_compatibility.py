#!/usr/bin/env python3
"""
Simple Web Crawler Test - Windows Compatible

This script tests the crawler functionality without using Twisted reactor,
which can have compatibility issues on Windows.
"""

import sys
import os
import requests
from datetime import datetime
from HTMLCrawler.items import CrawlItem
from HTMLCrawler.pipelines import KeywordMatchingPipeline, DataExportPipeline

def test_basic_crawling():
    """Test basic crawling functionality without Scrapy reactor"""
    print("Testing basic web crawling functionality...")
    
    try:
        # Test with a simple HTTP request (simulating what Scrapy would do)
        url = "http://quotes.toscrape.com"
        print(f"Testing connection to: {url}")
        
        response = requests.get(url, timeout=10)
        if response.status_code == 200:
            print("‚úÖ Successfully connected to test site")
            
            # Create a mock CrawlItem
            item = CrawlItem()
            item['url'] = url
            item['title'] = "Test Page"
            item['content'] = response.text[:1000]  # First 1000 chars
            item['matched_keywords'] = []
            item['links_found'] = []
            item['depth'] = 0
            item['crawl_time'] = datetime.now().isoformat()
            item['loading_time'] = response.elapsed.total_seconds()
            
            print("‚úÖ Successfully created crawl item")
            
            # Test keyword matching pipeline
            keywords = ['python', 'web', 'quotes', 'scrapy']
            pipeline = KeywordMatchingPipeline(keywords)
            
            class MockSpider:
                def __init__(self):
                    self.keywords = keywords
                    self.filter_by_keywords = False
            
            mock_spider = MockSpider()
            processed_item = pipeline.process_item(item, mock_spider)
            print(f"‚úÖ Keyword matching successful - found keywords: {processed_item.get('matched_keywords', [])}")
            
            return True
            
    except requests.RequestException as e:
        print(f"‚ùå Network test failed: {e}")
        print("‚ÑπÔ∏è  This is normal if you don't have internet connection")
        return True  # Don't fail the test for network issues
    except Exception as e:
        print(f"‚ùå Crawling test failed: {e}")
        return False

def test_data_export():
    """Test data export functionality"""
    print("\nTesting data export...")
    
    try:
        # Create test items
        items = []
        for i in range(3):
            item = CrawlItem()
            item['url'] = f"http://example.com/page{i}"
            item['title'] = f"Test Page {i}"
            item['content'] = f"This is test content for page {i} with python and web development keywords."
            item['matched_keywords'] = ['python', 'web']
            item['links_found'] = [f"http://example.com/link{i}"]
            item['depth'] = i
            item['crawl_time'] = datetime.now().isoformat()
            item['loading_time'] = 0.5
            items.append(item)
        
        # Test export pipeline
        class MockSpider:
            output_dir = "test_output"
        
        pipeline = DataExportPipeline()
        pipeline.open_spider(MockSpider())
        
        for item in items:
            pipeline.process_item(item, MockSpider())
        
        pipeline.close_spider(MockSpider())
        
        # Check if files were created
        import glob
        csv_files = glob.glob("test_output/crawl_results_*.csv")
        json_files = glob.glob("test_output/crawl_results_*.json")
        
        if csv_files and json_files:
            print("‚úÖ Data export successful - CSV and JSON files created")
            print(f"   CSV file: {csv_files[0]}")
            print(f"   JSON file: {json_files[0]}")
            return True
        else:
            print("‚ùå Data export failed - no output files created")
            return False
            
    except Exception as e:
        print(f"‚ùå Export test failed: {e}")
        return False

def main():
    """Run all tests"""
    print("=" * 50)
    print("WINDOWS-COMPATIBLE CRAWLER TEST")
    print("=" * 50)
    
    success = True
    
    # Test basic functionality
    success &= test_basic_crawling()
    
    # Test data export
    success &= test_data_export()
    
    print("\n" + "=" * 50)
    if success:
        print("üéâ All Windows compatibility tests passed!")
        print("\nThe crawler components are working correctly.")
        print("Note: Full Scrapy crawling may have Windows-specific issues")
        print("with the Twisted reactor, but the core functionality works.")
    else:
        print("‚ùå Some tests failed")
    print("=" * 50)
    
    return 0 if success else 1

if __name__ == '__main__':
    sys.exit(main())